{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ae4799-5cb6-4656-83cf-cad65997eae9",
   "metadata": {},
   "source": [
    "<h1><center>Feature Extraction </center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bbaa1d0-6426-473b-8d4d-7fa2266fe8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "439af61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"dementia_dataset_4.csv\")\n",
    "df[\"combined_text\"] = df[\"Transcript_CTD\"].fillna(\"\").astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ec8e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995d94c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of Class_label before clustering:\n",
      "Class_label\n",
      "1    415\n",
      "0    360\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the counts of Class_label before clustering\n",
    "class_label_counts = df['Class_label'].value_counts()\n",
    "print(\"Counts of Class_label before clustering:\")\n",
    "print(class_label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f556658e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(775, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ac7bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature functions\n",
    "\n",
    "def calculate_ttr(text):\n",
    "    words = text.split()\n",
    "    return len(set(words)) / len(words) if words else 0\n",
    "\n",
    "def calculate_brunet(text):\n",
    "    words = text.split()\n",
    "    return len(words) ** (len(set(words)) ** -0.165) if words else 0\n",
    "\n",
    "def calculate_avg_word_length(text):\n",
    "    words = text.split()\n",
    "    return np.mean([len(word) for word in words]) if words else 0\n",
    "\n",
    "def calculate_pos_ratios(text):\n",
    "    doc = nlp(text)\n",
    "    total = len(doc)\n",
    "    counts = Counter([t.pos_ for t in doc])\n",
    "    return {\n",
    "        \"NOUN_ratio\": counts.get(\"NOUN\", 0) / total if total else 0,\n",
    "        \"VERB_ratio\": counts.get(\"VERB\", 0) / total if total else 0,\n",
    "        \"PRONOUN_ratio\": counts.get(\"PRON\", 0) / total if total else 0,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142b158d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parse_depth(sent):\n",
    "    depths = {token.i: 0 for token in sent}\n",
    "    for token in sent:\n",
    "        if token.head != token:\n",
    "            depths[token.i] = depths[token.head.i] + 1\n",
    "    return max(depths.values()) if depths else 0\n",
    "\n",
    "def calculate_sentence_complexity(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    if not sentences:\n",
    "        return {\"subordinate_clauses\": 0, \"parse_tree_depth\": 0}\n",
    "    clauses = sum(1 for t in doc if t.dep_ == \"mark\")\n",
    "    depth = max(get_parse_depth(s) for s in sentences)\n",
    "    return {\"subordinate_clauses\": clauses / len(sentences), \"parse_tree_depth\": depth}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1f17c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_idea_density(text):\n",
    "    words = text.split()\n",
    "    return (len(set(words)) / len(words)) * 100 if words else 0\n",
    "\n",
    "def count_key_elements(text, elements):\n",
    "    text = text.lower()\n",
    "    return sum(1 for e in elements if e in text)\n",
    "\n",
    "def count_irrelevant_details(text, details):\n",
    "    text = text.lower()\n",
    "    return sum(1 for d in details if d in text)\n",
    "\n",
    "def count_pauses(text):\n",
    "    return len(re.findall(r\"\\b(uh|um)\\b\", text.lower()))\n",
    "\n",
    "def calculate_repair_rate(text):\n",
    "    doc = nlp(text)\n",
    "    repairs = sum(1 for token in doc if token.dep_ == \"reparandum\")\n",
    "    return repairs / len(list(doc.sents)) if doc else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c50532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply features to full dataset\n",
    "df[\"TTR\"] = df[\"combined_text\"].apply(calculate_ttr)\n",
    "df[\"Brunet_Index\"] = df[\"combined_text\"].apply(calculate_brunet)\n",
    "df[\"Avg_Word_Length\"] = df[\"combined_text\"].apply(calculate_avg_word_length)\n",
    "\n",
    "pos_ratios = df[\"combined_text\"].apply(calculate_pos_ratios)\n",
    "df[\"NOUN_ratio\"] = pos_ratios.apply(lambda x: x[\"NOUN_ratio\"])\n",
    "df[\"VERB_ratio\"] = pos_ratios.apply(lambda x: x[\"VERB_ratio\"])\n",
    "df[\"PRONOUN_ratio\"] = pos_ratios.apply(lambda x: x[\"PRONOUN_ratio\"])\n",
    "\n",
    "complexity = df[\"combined_text\"].apply(calculate_sentence_complexity)\n",
    "df[\"Subordinate_Clauses\"] = complexity.apply(lambda x: x[\"subordinate_clauses\"] if x else 0)\n",
    "df[\"Parse_Tree_Depth\"] = complexity.apply(lambda x: x[\"parse_tree_depth\"] if x else 0)\n",
    "\n",
    "df[\"Idea_Density\"] = df[\"combined_text\"].apply(calculate_idea_density)\n",
    "\n",
    "key_elements = [\"boy stealing cookies\", \"sink overflowing\", \"mother\", \"kitchen\", \"cookies\"]\n",
    "irrelevant_details = [\"dog\", \"cat\", \"car\", \"tree\"]\n",
    "\n",
    "df[\"Key_Elements_Described\"] = df[\"combined_text\"].apply(lambda x: count_key_elements(x, key_elements))\n",
    "df[\"Irrelevant_Details\"] = df[\"combined_text\"].apply(lambda x: count_irrelevant_details(x, irrelevant_details))\n",
    "df[\"Pauses\"] = df[\"combined_text\"].apply(count_pauses)\n",
    "df[\"Repair_Rate\"] = df[\"combined_text\"].apply(calculate_repair_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32fda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"combined_text\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "488d4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction done and saved as 'dementia_dataset_5.csv'!\n"
     ]
    }
   ],
   "source": [
    "# Save processed dataset\n",
    "df.to_csv(\"dementia_dataset_5.csv\", index=False)\n",
    "print(\"Feature extraction done and saved as 'dementia_dataset_5.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddb538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
